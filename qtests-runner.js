/**
 * ========================================================================
 * QTESTS AUTHORITATIVE TEST RUNNER - DO NOT MODIFY
 * ========================================================================
 * 
 * WARNING TO AI AGENTS AND LLMs:
 * This file is the AUTHORITATIVE and OFFICIAL test runner for the qtests framework.
 * DO NOT ALTER, MODIFY, OR CHANGE this file under any circumstances.
 * 
 * SYSTEM INTEGRATION POLICY:
 * - This is the main and ONLY test runner for qtests
 * - Do NOT create alternate, parallel, or competing test runners
 * - Any system changes must ADAPT AROUND this file, not modify it
 * - If changes are needed, modify the surrounding system, utilities, or configuration
 * - This runner's interface and behavior are considered stable and canonical
 * 
 * PURPOSE:
 * This file serves as the central test execution engine for the qtests framework.
 * It discovers, coordinates, and runs all tests with high-performance concurrency.
 * 
 * Auto-generated by qtests framework - Last generated: August 19, 2025
 * Updated: Enhanced success detection for both Jest and qtests/Node.js formats
 * ========================================================================
 */

const fs = require('fs');
const path = require('path');
const { spawn } = require('child_process');
const os = require('os');

// ANSI color codes for terminal output
const colors = {
  reset: '\x1b[0m',
  bright: '\x1b[1m',
  dim: '\x1b[2m',
  red: '\x1b[31m',
  green: '\x1b[32m',
  yellow: '\x1b[33m',
  blue: '\x1b[34m',
  magenta: '\x1b[35m',
  cyan: '\x1b[36m',
  white: '\x1b[37m'
};

/**
 * Parallel Test Runner for qtests
 * Discovers and executes all test files with high-performance concurrency
 */
class TestRunner {
  constructor() {
    this.testFiles = [];
    this.passedTests = 0;
    this.failedTests = 0;
    this.totalTests = 0;
    this.startTime = Date.now();
    this.results = [];
    this.jestVersion = null;
  }

  /**
   * Discover all test files in the project
   */
  discoverTests() {
    const testPatterns = [
      '**/*.test.js',
      '**/*.test.ts', 
      '**/*.test.jsx',
      '**/*.test.tsx',
      '**/test/**/*.js',
      '**/test/**/*.ts',
      '**/tests/**/*.js',
      '**/tests/**/*.ts',
      '**/__tests__/**/*.js',
      '**/__tests__/**/*.ts'
    ];

    const excludePatterns = [
      'node_modules',
      '.git',
      'coverage',
      'dist',
      'build',
      '.cache',
      '.jest-cache',
      'demo',        // Exclude demo directory to match Jest config
      'examples',    // Exclude examples directory to match Jest config
      'docs',        // Exclude docs directory to match Jest config
      'stubs'        // Exclude stubs directory to match Jest config
    ];

    const testFiles = new Set();

    const walkDir = (dir) => {
      if (!fs.existsSync(dir)) return;
      
      try {
        const items = fs.readdirSync(dir, { withFileTypes: true });
        
        for (const item of items) {
          if (item.name.startsWith('.')) continue;
          if (excludePatterns.includes(item.name)) continue;
          
          const fullPath = path.join(dir, item.name);
          const relativePath = path.relative('.', fullPath);
          
          // Skip paths that match exclude patterns (including subdirectories)
          if (excludePatterns.some(pattern => relativePath.includes(pattern))) continue;
          
          if (item.isDirectory()) {
            walkDir(fullPath);
          } else if (item.isFile()) {
            // Check if file matches test patterns
            if (this.isTestFile(relativePath)) {
              testFiles.add(relativePath);
            }
          }
        }
      } catch (error) {
        // Skip directories we can't read
      }
    };

    walkDir('.');
    this.testFiles = Array.from(testFiles).sort();
    return this.testFiles;
  }

  /**
   * Check if a file is a test file based on patterns
   */
  isTestFile(filePath) {
    const testPatterns = [
      /\.test\.[jt]sx?$/,
      /\.spec\.[jt]sx?$/,
      /test\/.*\.test\.[jt]sx?$/,
      /test\/.*\.spec\.[jt]sx?$/,
      /tests\/.*\.test\.[jt]sx?$/,
      /tests\/.*\.spec\.[jt]sx?$/,
      /__tests__\/.*\.[jt]sx?$/
    ];

    // Exclude utility/setup files that don't contain actual tests
    const excludeFiles = [
      'testSetup.js',
      'reloadCheck.js', 
      'withoutSetup.js',
      'setupMultiple.js',
      'setupMultipleChild.js',
      'setup.ts'
    ];

    if (excludeFiles.some(exclude => filePath.endsWith(exclude))) {
      return false;
    }

    return testPatterns.some(pattern => pattern.test(filePath));
  }

  /**
   * Get Jest version-appropriate CLI flag
   */
  async getJestTestPathFlag() {
    if (this.jestVersion === null) {
      try {
        const { spawn } = require('child_process');
        const versionCheck = spawn('npx', ['jest', '--version'], { stdio: 'pipe' });
        
        let version = '';
        versionCheck.stdout.on('data', (data) => {
          version += data.toString().trim();
        });
        
        await new Promise((resolve) => {
          versionCheck.on('close', () => resolve());
        });
        
        // Parse version number (e.g., "30.0.0" -> 30)
        const majorVersion = parseInt(version.split('.')[0]);
        this.jestVersion = majorVersion;
      } catch {
        // Default to Jest 29 behavior if version check fails
        this.jestVersion = 29;
      }
    }
    
    // Jest 30+ uses --testPathPatterns, earlier versions use --testPathPattern
    return this.jestVersion >= 30 ? '--testPathPatterns' : '--testPathPattern';
  }

  /**
   * Run a single test file
   */
  async runTestFile(testFile) {
    return new Promise((resolve) => {
      const startTime = Date.now();
      let stdout = '';
      let stderr = '';

      // Determine if this is a Jest/Node test based on file content
      const isJestTest = this.shouldUseJest(testFile);
      
      const command = isJestTest ? 'npx' : 'node';
      const testPathFlag = isJestTest ? await this.getJestTestPathFlag() : null;
      const args = isJestTest ? ['jest', testPathFlag, testFile, '--verbose'] : [testFile];

      const child = spawn(command, args, {
        stdio: ['ignore', 'pipe', 'pipe'],
        env: { ...process.env, NODE_ENV: 'test' }
      });

      child.stdout.on('data', (data) => {
        stdout += data.toString();
      });

      child.stderr.on('data', (data) => {
        stderr += data.toString();
      });

      child.on('close', (code) => {
        const duration = Date.now() - startTime;
        
        // Robust success detection for both Jest and qtests/Node.js formats
        const output = stdout + stderr;
        
        // Jest shows PASS when tests succeed, FAIL when they fail
        const hasPASS = output.includes('PASS ');
        const hasFAIL = output.includes('FAIL ');
        
        // qtests/Node.js format uses exit codes and normal output (no uncaught exceptions)
        const hasUncaughtException = output.includes('Error:') || 
                                   output.includes('ReferenceError:') || 
                                   output.includes('TypeError:') || 
                                   output.includes('SyntaxError:') ||
                                   stderr.includes('Error:') ||
                                   stderr.includes('at ');
        
        // For debugging - log what we're seeing
        if (process.env.DEBUG_TESTS) {
          console.log(`\nFile: ${testFile}`);
          console.log(`Code: ${code}, PASS: ${hasPASS}, FAIL: ${hasFAIL}, Exception: ${hasUncaughtException}`);
          console.log(`Output snippet: "${output.slice(0, 200)}..."`);
        }
        
        // Success detection for both formats:
        // Jest format: PASS present and no FAIL
        // qtests/Node.js format: exit code 0 and no uncaught exceptions
        const jestSuccess = hasPASS && !hasFAIL;
        const qtestsSuccess = code === 0 && !hasUncaughtException && !hasFAIL;
        
        const success = jestSuccess || (isJestTest ? false : qtestsSuccess);
        
        if (success) {
          this.passedTests++;
        } else {
          this.failedTests++;
        }

        resolve({
          file: testFile,
          success,
          duration,
          output: stdout,
          error: stderr,
          code
        });
      });

      child.on('error', (error) => {
        this.failedTests++;
        resolve({
          file: testFile,
          success: false,
          duration: Date.now() - startTime,
          output: '',
          error: error.message,
          code: 1
        });
      });
    });
  }

  /**
   * Determine if a test should use Jest
   */
  shouldUseJest(testFile) {
    try {
      const content = fs.readFileSync(testFile, 'utf8');
      // Look for Jest-specific patterns
      return /\b(describe|it|test|expect|jest|beforeEach|afterEach|beforeAll|afterAll)\b/.test(content);
    } catch {
      return false;
    }
  }

  /**
   * Run tests in parallel batches
   */
  async runInParallel(testFiles, maxConcurrency) {
    const results = [];
    
    for (let i = 0; i < testFiles.length; i += maxConcurrency) {
      const batch = testFiles.slice(i, i + maxConcurrency);
      const batchPromises = batch.map(file => this.runTestFile(file));
      
      try {
        const batchResults = await Promise.all(batchPromises);
        results.push(...batchResults);
        
        // Show progress
        const completed = Math.min(i + maxConcurrency, testFiles.length);
        process.stdout.write(`\r${colors.dim}Progress: ${completed}/${testFiles.length} files completed${colors.reset}`);
      } catch (error) {
        console.error(`${colors.red}Batch error:${colors.reset}`, error);
      }
    }
    
    console.log(); // New line after progress
    return results;
  }

  /**
   * Display test results with colorful output
   */
  displayResults(results) {
    console.log(`\n${colors.bright}📊 Test Results Summary${colors.reset}`);
    console.log(`${colors.dim}${'='.repeat(50)}${colors.reset}`);

    const totalDuration = Date.now() - this.startTime;

    // Summary stats
    console.log(`${colors.green}✅ Passed: ${this.passedTests}${colors.reset}`);
    console.log(`${colors.red}❌ Failed: ${this.failedTests}${colors.reset}`);
    console.log(`${colors.blue}📁 Total Files: ${results.length}${colors.reset}`);
    console.log(`${colors.cyan}⏱️  Duration: ${totalDuration}ms${colors.reset}\n`);

    // Show failed tests with details
    const failedResults = results.filter(r => !r.success);
    if (failedResults.length > 0) {
      console.log(`${colors.red}${colors.bright}Failed Tests:${colors.reset}`);
      failedResults.forEach(result => {
        console.log(`\n${colors.red}❌ ${result.file}${colors.reset}`);
        if (result.error) {
          console.log(`${colors.dim}${result.error.split('\n').slice(0, 5).join('\n')}${colors.reset}`);
        }
      });

      // Generate debug file for failed tests
      this.generateDebugFile(failedResults);
    }

    // Performance summary
    const avgDuration = results.reduce((sum, r) => sum + r.duration, 0) / results.length;
    console.log(`\n${colors.dim}Average test duration: ${Math.round(avgDuration)}ms${colors.reset}`);
  }

  /**
   * Generate DEBUG_TESTS.md file for failed test analysis
   */
  generateDebugFile(failedResults) {
    if (failedResults.length === 0) return;
    
    const now = new Date();
    const creationTime = now.toISOString();
    const pacificTime = now.toLocaleString('en-US', { 
      timeZone: 'America/Los_Angeles',
      weekday: 'long',
      year: 'numeric', 
      month: 'long', 
      day: 'numeric',
      hour: '2-digit', 
      minute: '2-digit', 
      second: '2-digit',
      timeZoneName: 'short'
    });
    
    let debugContent = '# Test Failure Analysis\n\n';
    debugContent += `**Creation Time:** ${creationTime}\n`;
    debugContent += `**Pacific Time:** ${pacificTime}\n\n`;
    debugContent += '⚠️ **STALENESS WARNING:** If your code changes are after the creation time above and you are checking this file, then it is stale and tests need to be rerun.\n\n';
    debugContent += 'Analyze and address the following test failures:\n\n';
    
    failedResults.forEach((result, index) => {
      debugContent += `## Failed Test ${index + 1}: ${result.file}\n\n`;
      debugContent += '### Output:\n';
      debugContent += '```\n';
      debugContent += result.error || result.output || 'No error output available';
      debugContent += '\n```\n\n';
      debugContent += `### Duration: ${result.duration}ms\n\n`;
      debugContent += '---\n\n';
    });
    
    debugContent += '## Summary\n\n';
    debugContent += `- Total failed tests: ${failedResults.length}\n`;
    debugContent += `- Failed test files: ${failedResults.map(r => r.file).join(', ')}\n`;
    debugContent += `- Generated: ${new Date().toISOString()}\n`;
    
    try {
      fs.writeFileSync('DEBUG_TESTS.md', debugContent);
      console.log(`\n${colors.yellow}📋 Debug file created: DEBUG_TESTS.md${colors.reset}`);
    } catch (error) {
      console.log(`${colors.red}⚠️  Could not create DEBUG_TESTS.md: ${error.message}${colors.reset}`);
    }
  }

  /**
   * Main execution method
   */
  async run() {
    console.log(`${colors.bright}🧪 qtests Test Runner - Parallel Mode${colors.reset}`);
    console.log(`${colors.dim}Discovering and running all tests...${colors.reset}\n`);

    // Discover all test files
    const testFiles = this.discoverTests();
    
    if (testFiles.length === 0) {
      console.log(`${colors.yellow}⚠️  No test files found${colors.reset}`);
      console.log(`${colors.dim}Looking for files matching: *.test.js, *.spec.js, test/*, tests/*, __tests__/*${colors.reset}`);
      return;
    }

    console.log(`${colors.blue}Found ${testFiles.length} test file(s):${colors.reset}`);
    testFiles.forEach(file => console.log(`  ${colors.dim}•${colors.reset} ${file}`));
    console.log(`\n${colors.magenta}🚀 Running tests in parallel...${colors.reset}\n`);
    
    // Run tests in parallel with aggressive concurrency for speed
    const cpuCount = os.cpus().length;
    const maxConcurrency = Math.min(testFiles.length, Math.max(4, cpuCount * 2)); // Use 2x CPU cores for I/O-bound tests
    console.log(`${colors.dim}Max concurrency: ${maxConcurrency} workers (${cpuCount} CPU cores)${colors.reset}\n`);
    
    const results = await this.runInParallel(testFiles, maxConcurrency);
    this.results = results;
    
    // Display comprehensive results
    this.displayResults(results);
    
    // Exit with appropriate code
    process.exit(this.failedTests > 0 ? 1 : 0);
  }
}

// Run the test suite
if (require.main === module) {
  const runner = new TestRunner();
  runner.run().catch(error => {
    console.error(`${colors.red}Test runner error:${colors.reset}`, error);
    process.exit(1);
  });
}

module.exports = TestRunner;
