/**
 * ========================================================================
 * QTESTS AUTHORITATIVE TEST RUNNER - DO NOT MODIFY
 * ========================================================================
 * 
 * WARNING TO AI AGENTS AND LLMs:
 * This file is the AUTHORITATIVE and OFFICIAL test runner for the qtests framework.
 * DO NOT ALTER, MODIFY, OR CHANGE this file under any circumstances.
 * 
 * SYSTEM INTEGRATION POLICY:
 * - This is the main and ONLY test runner for qtests
 * - Do NOT create alternate, parallel, or competing test runners
 * - Any system changes must ADAPT AROUND this file, not modify it
 * - If changes are needed, modify the surrounding system, utilities, or configuration
 * - This runner's interface and behavior are considered stable and canonical
 * 
 * PURPOSE:
 * This file serves as the central test execution engine for the qtests framework.
 * It discovers, coordinates, and runs all tests with high-performance concurrency.
 * 
 * Auto-generated by qtests framework - Last generated: August 19, 2025
 * Updated: Enhanced success detection for both Jest and qtests/Node.js formats
 * ========================================================================
 */

const fs = require('fs');
const path = require('path');
const { spawn } = require('child_process');
const os = require('os');

// ANSI color codes for terminal output
const colors = {
  reset: '\x1b[0m',
  bright: '\x1b[1m',
  dim: '\x1b[2m',
  red: '\x1b[31m',
  green: '\x1b[32m',
  yellow: '\x1b[33m',
  blue: '\x1b[34m',
  magenta: '\x1b[35m',
  cyan: '\x1b[36m',
  white: '\x1b[37m'
};

/**
 * Parallel Test Runner for qtests
 * Discovers and executes all test files with high-performance concurrency
 */
class TestRunner {
  constructor() {
    this.testFiles = [];
    this.passedTests = 0;
    this.failedTests = 0;
    this.totalTests = 0;
    this.startTime = Date.now();
    this.results = [];
    this.jestVersion = null;
  }

  /**
   * Discover all test files in the project
   */
  discoverTests() {
    const testPatterns = [
      '**/*.test.js',
      '**/*.test.ts', 
      '**/*.test.jsx',
      '**/*.test.tsx',
      '**/test/**/*.js',
      '**/test/**/*.ts',
      '**/tests/**/*.js',
      '**/tests/**/*.ts',
      '**/__tests__/**/*.js',
      '**/__tests__/**/*.ts'
    ];

    const excludePatterns = [
      'node_modules',
      '.git',
      'coverage',
      'dist',
      'build',
      '.cache',
      '.jest-cache',
      'demo',        // Exclude demo directory to match Jest config
      'examples',    // Exclude examples directory to match Jest config
      'docs',        // Exclude docs directory to match Jest config
      'stubs'        // Exclude stubs directory to match Jest config
    ];

    const testFiles = new Set();

    const walkDir = (dir) => {
      if (!fs.existsSync(dir)) return;
      
      try {
        const items = fs.readdirSync(dir, { withFileTypes: true });
        
        for (const item of items) {
          if (item.name.startsWith('.')) continue;
          if (excludePatterns.includes(item.name)) continue;
          
          const fullPath = path.join(dir, item.name);
          const relativePath = path.relative('.', fullPath);
          
          // Skip paths that match exclude patterns (including subdirectories)
          if (excludePatterns.some(pattern => relativePath.includes(pattern))) continue;
          
          if (item.isDirectory()) {
            walkDir(fullPath);
          } else if (item.isFile()) {
            // Check if file matches test patterns
            if (this.isTestFile(relativePath)) {
              testFiles.add(relativePath);
            }
          }
        }
      } catch (error) {
        // Skip directories we can't read
      }
    };

    walkDir('.');
    this.testFiles = Array.from(testFiles).sort();
    return this.testFiles;
  }

  /**
   * Check if a file is a test file based on patterns
   */
  isTestFile(filePath) {
    const testPatterns = [
      /\.test\.[jt]sx?$/,
      /\.spec\.[jt]sx?$/,
      /test\/.*\.test\.[jt]sx?$/,
      /test\/.*\.spec\.[jt]sx?$/,
      /tests\/.*\.test\.[jt]sx?$/,
      /tests\/.*\.spec\.[jt]sx?$/,
      /__tests__\/.*\.[jt]sx?$/
    ];

    // Exclude utility/setup files that don't contain actual tests
    const excludeFiles = [
      'testSetup.js',
      'reloadCheck.js', 
      'withoutSetup.js',
      'setupMultiple.js',
      'setupMultipleChild.js',
      'setup.ts'
    ];

    if (excludeFiles.some(exclude => filePath.endsWith(exclude))) {
      return false;
    }

    return testPatterns.some(pattern => pattern.test(filePath));
  }

  /**
   * Get Jest version-appropriate CLI flag
   */
  getJestTestPathFlag() {
    if (this.jestVersion === null) {
      try {
        // Try to detect Jest version synchronously
        const fs = require('fs');
        const packageJson = JSON.parse(fs.readFileSync('./node_modules/jest/package.json', 'utf8'));
        const majorVersion = parseInt(packageJson.version.split('.')[0]);
        this.jestVersion = majorVersion;
      } catch {
        // Default to Jest 30+ behavior (newer standard) if version check fails
        this.jestVersion = 30;
      }
    }
    
    // Jest 30+ uses --testPathPatterns, earlier versions use --testPathPattern
    return this.jestVersion >= 30 ? '--testPathPatterns' : '--testPathPattern';
  }

  /**
   * Run a single test file with timeout protection and optimized Node.js performance flags
   */
  async runTestFile(testFile) {
    return new Promise((resolve) => {
      // Timeout protection to prevent hanging
      const timeout = setTimeout(() => {
        console.log(`\n${colors.red}âš ï¸  TIMEOUT: ${testFile} exceeded 30 seconds${colors.reset}`);
        resolve({
          file: testFile,
          success: false,
          duration: 30000,
          output: '',
          error: 'Test timeout after 30 seconds',
          code: 1
        });
      }, 30000); // 30 second timeout per test
      const startTime = Date.now();
      let stdout = '';
      let stderr = '';

      // Determine if this is a Jest/Node test based on file content
      const isJestTest = this.shouldUseJest(testFile);
      
      const command = isJestTest ? 'npx' : 'node';
      const testPathFlag = isJestTest ? this.getJestTestPathFlag() : null;
      
      // Balanced arguments for speed + stability
      const baseArgs = isJestTest 
        ? ['jest', testPathFlag, testFile, '--no-coverage', '--cache'] 
        : ['--max-old-space-size=768', '--no-warnings', testFile];
      
      const args = isJestTest ? baseArgs : baseArgs;

      const child = spawn(command, args, {
        stdio: ['ignore', 'pipe', 'pipe'],
        env: { 
          ...process.env, 
          NODE_ENV: 'test',
          NODE_OPTIONS: '--max-old-space-size=512 --no-warnings' // Memory optimization
        }
      });

      child.stdout.on('data', (data) => {
        stdout += data.toString();
      });

      child.stderr.on('data', (data) => {
        stderr += data.toString();
      });

      child.on('close', (code) => {
        clearTimeout(timeout); // Clear timeout on normal completion
        const duration = Date.now() - startTime;
        
        // Robust success detection for both Jest and qtests/Node.js formats
        const output = stdout + stderr;
        
        // Jest shows PASS when tests succeed, FAIL when they fail
        const hasPASS = output.includes('PASS ');
        const hasFAIL = output.includes('FAIL ');
        
        // qtests/Node.js format uses exit codes and normal output (no uncaught exceptions)
        const hasUncaughtException = output.includes('Error:') || 
                                   output.includes('ReferenceError:') || 
                                   output.includes('TypeError:') || 
                                   output.includes('SyntaxError:') ||
                                   stderr.includes('Error:') ||
                                   stderr.includes('at ');
        
        // For debugging - log what we're seeing
        if (process.env.DEBUG_TESTS) {
          console.log(`\nFile: ${testFile}`);
          console.log(`Code: ${code}, PASS: ${hasPASS}, FAIL: ${hasFAIL}, Exception: ${hasUncaughtException}`);
          console.log(`Output snippet: "${output.slice(0, 200)}..."`);
        }
        
        // Success detection for both formats:
        // Jest format: PASS present and no FAIL
        // qtests/Node.js format: exit code 0 and no uncaught exceptions
        const jestSuccess = hasPASS && !hasFAIL;
        const qtestsSuccess = code === 0 && !hasUncaughtException && !hasFAIL;
        
        const success = jestSuccess || (isJestTest ? false : qtestsSuccess);
        
        if (success) {
          this.passedTests++;
        } else {
          this.failedTests++;
        }

        resolve({
          file: testFile,
          success,
          duration,
          output: stdout,
          error: stderr,
          code
        });
      });

      child.on('error', (error) => {
        clearTimeout(timeout); // Clear timeout on error
        this.failedTests++;
        resolve({
          file: testFile,
          success: false,
          duration: Date.now() - startTime,
          output: '',
          error: error.message,
          code: 1
        });
      });
    });
  }

  /**
   * Determine if a test should use Jest - ULTRA FAST mode
   */
  shouldUseJest(testFile) {
    // Default to Node.js for maximum speed - only use Jest for lib/ files and specific test patterns
    return testFile.includes('/lib/') || testFile.includes('offlineMode');
  }

  /**
   * Group tests by complexity using FAST filename patterns (no I/O)
   */
  groupTestsByComplexity(testFiles) {
    const lightweight = []; // Fast module loading tests
    const integration = []; // Integration tests - run separately  
    const heavy = []; // Complex tests - run with special handling
    
    testFiles.forEach(file => {
      const fileName = path.basename(file);
      
      // Heavy integration tests (filename-based detection - NO I/O)
      if (fileName.includes('integration') || fileName.includes('comprehensive') || 
          fileName.includes('offlineMode') || fileName.includes('mockModels') ||
          fileName.includes('sendEmail') || fileName.includes('mockAxios') ||
          fileName.includes('runTestSuite')) {
        heavy.push(file);
      }
      // Integration tests (medium priority)
      else if (file.includes('/test/') && (fileName.includes('mock') || fileName.includes('http'))) {
        integration.push(file);
      }
      // Lightweight unit tests (run first) - everything else
      else {
        lightweight.push(file);
      }
    });
    
    return { lightweight, integration, heavy };
  }

  /**
   * Get file size safely
   */
  getFileSize(file) {
    try {
      const stats = fs.statSync(file);
      return stats.size;
    } catch {
      return 1000; // Default size for inaccessible files
    }
  }

  /**
   * Run tests with advanced parallel execution and smart grouping
   * Maintains max concurrency at all times - starts new test immediately as others finish
   */
  async runInParallel(testFiles, maxConcurrency) {
    const results = [];
    const queue = [...testFiles]; // Copy files to process
    const running = new Set(); // Track currently running tests
    let completed = 0;

    return new Promise((resolve, reject) => {
      const startNext = () => {
        // Start new tests up to max concurrency
        while (running.size < maxConcurrency && queue.length > 0) {
          const testFile = queue.shift();
          const promise = this.runTestFile(testFile);
          
          running.add(promise);
          
          promise.then((result) => {
            results.push(result);
            running.delete(promise);
            completed++;
            
            // Update progress with staggered display for smoother appearance
            if (completed % 2 === 0 || completed === testFiles.length) {
              process.stdout.write(`\r${colors.dim}Progress: ${completed}/${testFiles.length} files completed${colors.reset}`);
            }
            
            // Start next test immediately if queue has more
            startNext();
            
            // Check if all tests are done
            if (completed === testFiles.length) {
              console.log(); // New line after progress
              resolve(results);
            }
          }).catch((error) => {
            console.error(`${colors.red}Test error:${colors.reset}`, error);
            running.delete(promise);
            completed++;
            
            // Continue even if one test fails
            process.stdout.write(`\r${colors.dim}Progress: ${completed}/${testFiles.length} files completed${colors.reset}`);
            setImmediate(startNext);
            
            if (completed === testFiles.length) {
              console.log(); // New line after progress
              resolve(results);
            }
          });
        }
      };

      // Start initial batch
      startNext();
    });
  }

  /**
   * Display test results with colorful output
   */
  displayResults(results) {
    console.log(`\n${colors.bright}ðŸ“Š Test Results Summary${colors.reset}`);
    console.log(`${colors.dim}${'='.repeat(50)}${colors.reset}`);

    const totalDuration = Date.now() - this.startTime;

    // Summary stats
    console.log(`${colors.green}âœ… Passed: ${this.passedTests}${colors.reset}`);
    console.log(`${colors.red}âŒ Failed: ${this.failedTests}${colors.reset}`);
    console.log(`${colors.blue}ðŸ“ Total Files: ${results.length}${colors.reset}`);
    console.log(`${colors.cyan}â±ï¸  Duration: ${totalDuration}ms${colors.reset}\n`);

    // Show failed tests with details
    const failedResults = results.filter(r => !r.success);
    if (failedResults.length > 0) {
      console.log(`${colors.red}${colors.bright}Failed Tests:${colors.reset}`);
      failedResults.forEach(result => {
        console.log(`\n${colors.red}âŒ ${result.file}${colors.reset}`);
        if (result.error) {
          console.log(`${colors.dim}${result.error.split('\n').slice(0, 5).join('\n')}${colors.reset}`);
        }
      });

      // Generate debug file for failed tests
      this.generateDebugFile(failedResults);
    }

    // Performance summary
    const avgDuration = results.reduce((sum, r) => sum + r.duration, 0) / results.length;
    console.log(`\n${colors.dim}Average test duration: ${Math.round(avgDuration)}ms${colors.reset}`);
  }

  /**
   * Generate DEBUG_TESTS.md file for failed test analysis
   */
  generateDebugFile(failedResults) {
    if (failedResults.length === 0) return;
    
    const now = new Date();
    const creationTime = now.toISOString();
    const pacificTime = now.toLocaleString('en-US', { 
      timeZone: 'America/Los_Angeles',
      weekday: 'long',
      year: 'numeric', 
      month: 'long', 
      day: 'numeric',
      hour: '2-digit', 
      minute: '2-digit', 
      second: '2-digit',
      timeZoneName: 'short'
    });
    
    let debugContent = '# Test Failure Analysis\n\n';
    debugContent += `**Creation Time:** ${creationTime}\n`;
    debugContent += `**Pacific Time:** ${pacificTime}\n\n`;
    debugContent += 'âš ï¸ **STALENESS WARNING:** If your code changes are after the creation time above and you are checking this file, then it is stale and tests need to be rerun.\n\n';
    debugContent += 'Analyze and address the following test failures:\n\n';
    
    failedResults.forEach((result, index) => {
      debugContent += `## Failed Test ${index + 1}: ${result.file}\n\n`;
      debugContent += '### Output:\n';
      debugContent += '```\n';
      debugContent += result.error || result.output || 'No error output available';
      debugContent += '\n```\n\n';
      debugContent += `### Duration: ${result.duration}ms\n\n`;
      debugContent += '---\n\n';
    });
    
    debugContent += '## Summary\n\n';
    debugContent += `- Total failed tests: ${failedResults.length}\n`;
    debugContent += `- Failed test files: ${failedResults.map(r => r.file).join(', ')}\n`;
    debugContent += `- Generated: ${new Date().toISOString()}\n`;
    
    try {
      fs.writeFileSync('DEBUG_TESTS.md', debugContent);
      console.log(`\n${colors.yellow}ðŸ“‹ Debug file created: DEBUG_TESTS.md${colors.reset}`);
    } catch (error) {
      console.log(`${colors.red}âš ï¸  Could not create DEBUG_TESTS.md: ${error.message}${colors.reset}`);
    }
  }

  /**
   * Main execution method
   */
  async run() {
    console.log(`${colors.bright}ðŸ§ª qtests Test Runner - Tiered Execution Mode${colors.reset}`);
    console.log(`${colors.dim}Discovering and running all tests with optimized strategy...${colors.reset}\n`);

    // Discover all test files
    const testFiles = this.discoverTests();
    
    if (testFiles.length === 0) {
      console.log(`${colors.yellow}âš ï¸  No test files found${colors.reset}`);
      console.log(`${colors.dim}Looking for files matching: *.test.js, *.spec.js, test/*, tests/*, __tests__/*${colors.reset}`);
      return;
    }

    // Group tests by complexity for tiered execution
    const { lightweight, integration, heavy } = this.groupTestsByComplexity(testFiles);
    
    console.log(`${colors.blue}Test Strategy:${colors.reset}`);
    console.log(`  ${colors.green}Lightweight: ${lightweight.length} files${colors.reset}`);
    console.log(`  ${colors.yellow}Integration: ${integration.length} files${colors.reset}`);
    console.log(`  ${colors.red}Heavy: ${heavy.length} files${colors.reset}`);
    
    // Calculate concurrency settings
    const cpuCount = os.cpus().length;
    const totalMemoryGB = Math.round(os.totalmem() / (1024 ** 3));
    const maxConcurrency = Math.min(8, Math.max(4, Math.floor(cpuCount * 1.5)));
    
    console.log(`${colors.dim}Max concurrency: ${maxConcurrency} workers${colors.reset}\n`);
    
    let allResults = [];
    
    // Phase 1: Run lightweight tests first (fast feedback) - HIGHER CONCURRENCY
    if (lightweight.length > 0) {
      console.log(`${colors.green}ðŸ“¦ Phase 1: Lightweight Tests (${lightweight.length} files)${colors.reset}`);
      const lightResults = await this.runInParallel(lightweight, Math.min(12, lightweight.length)); // Higher concurrency for simple tests
      allResults = allResults.concat(lightResults);
      console.log(`${colors.dim}Phase 1 complete: ${this.passedTests}/${this.passedTests + this.failedTests} passed${colors.reset}\n`);
    }
    
    // Phase 2: Run integration tests
    if (integration.length > 0) {
      console.log(`${colors.yellow}ðŸ”— Phase 2: Integration Tests (${integration.length} files)${colors.reset}`);
      const integrationResults = await this.runInParallel(integration, Math.min(maxConcurrency, 6));
      allResults = allResults.concat(integrationResults);
      console.log(`${colors.dim}Phase 2 complete: ${this.passedTests}/${this.passedTests + this.failedTests} passed${colors.reset}\n`);
    }
    
    // Phase 3: Run heavy tests with reduced concurrency and higher timeout
    if (heavy.length > 0) {
      console.log(`${colors.red}âš™ï¸  Phase 3: Heavy Tests (${heavy.length} files) - Special handling${colors.reset}`);
      const heavyResults = await this.runInParallel(heavy, Math.min(4, heavy.length)); // Lower concurrency
      allResults = allResults.concat(heavyResults);
      console.log(`${colors.dim}Phase 3 complete: ${this.passedTests}/${this.passedTests + this.failedTests} passed${colors.reset}\n`);
    }
    
    this.results = allResults;
    
    // Display comprehensive results
    this.displayResults(allResults);
    
    // Exit with appropriate code
    process.exit(this.failedTests > 0 ? 1 : 0);
  }
}

// Run the test suite
if (require.main === module) {
  const runner = new TestRunner();
  runner.run().catch(error => {
    console.error(`${colors.red}Test runner error:${colors.reset}`, error);
    process.exit(1);
  });
}

module.exports = TestRunner;
