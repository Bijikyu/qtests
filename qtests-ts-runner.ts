#!/usr/bin/env tsx
// Generated by qtests - Full-featured CLI tool for running tests
// This file is auto-generated by qtests but safe to customize

// qtests Test Runner - Full-featured CLI tool for running tests
// Generated by qtests - provides parallel execution, batching, and debug features

import fs from 'fs';
import path from 'path';
import { spawn } from 'child_process';
import os from 'os';

// ANSI color codes for terminal output
const colors = {
  green: '\u001b[32m',
  red: '\u001b[31m',
  yellow: '\u001b[33m',
  blue: '\u001b[34m',
  magenta: '\u001b[35m',
  cyan: '\u001b[36m',
  white: '\u001b[37m',
  reset: '\u001b[0m',
  bold: '\u001b[1m',
  dim: '\u001b[2m'
};

// Test discovery patterns
const TEST_PATTERNS = [
  /\.test\.(js|ts|jsx|tsx)$/,
  /\.spec\.(js|ts|jsx|tsx)$/,
  /_test\.(js|ts|jsx|tsx)$/,
  /_spec\.(js|ts|jsx|tsx)$/
];

class TestRunner {
  passedTests: number;
  failedTests: number;
  testResults: any[];
  startTime: number;

  constructor() {
    this.passedTests = 0;
    this.failedTests = 0;
    this.testResults = [];
    this.startTime = Date.now();
  }

  // Discover all test files in the project (from current working directory)
  discoverTests(dir = process.cwd(), depth = 0, maxDepth = 10): string[] {
    const testFiles: string[] = [];
    if (depth > maxDepth) return testFiles;
    try {
      const entries = fs.readdirSync(dir, { withFileTypes: true });
      for (const entry of entries) {
        const fullPath = path.join(dir, entry.name);
        // Skip node_modules, hidden directories, and demo directory
        if (entry.name.startsWith('.') || entry.name === 'node_modules' || entry.name === 'demo') {
          continue;
        }
        if (entry.isDirectory()) {
          testFiles.push(...this.discoverTests(fullPath, depth + 1, maxDepth));
        } else if (entry.isFile()) {
          const isTestFile = TEST_PATTERNS.some(pattern => pattern.test(entry.name));
          if (!isTestFile) continue;
          // Skip tests in __mocks__ and other ignored folders
          if (fullPath.includes(`${path.sep}__mocks__${path.sep}`)) continue;
          // In generated-tests, only include files that follow the GeneratedTest naming convention
          if (fullPath.includes(`${path.sep}generated-tests${path.sep}`) && !/GeneratedTest/.test(entry.name)) {
            continue;
          }
          testFiles.push(fullPath);
        }
      }
    } catch {
      // Silently skip directories we can't read
    }
    // Optional pattern filter via env to limit scope for large repos/CI
    const pattern = process.env.QTESTS_PATTERN;
    if (pattern) {
      try {
        const rx = new RegExp(pattern);
        return testFiles.filter(f => rx.test(f));
      } catch {
        return testFiles; // invalid pattern, ignore
      }
    }
    return testFiles;
  }

  // Run a single test file via Jest and capture output
  async runTestFile(testFile: string) {
    return new Promise((resolve) => {
      const startTime = Date.now();
      let stdout = '';
      let stderr = '';

      // Always use Jest with project config and allow empty matches
      const configCandidates = [
        path.join(process.cwd(), 'config', 'jest.config.mjs'),
        path.join(process.cwd(), 'jest.config.mjs')
      ];
      const cfg = configCandidates.find(p => {
        try { return fs.existsSync(p); } catch { return false; }
      });
      const jestArgs: string[] = [];
      if (cfg) {
        jestArgs.push('--config', cfg);
      }
      jestArgs.push('--passWithNoTests');
      // Target just this file for speed; keep small worker pool to reduce overhead
      jestArgs.push(testFile, '--maxWorkers=4', '--cache', '--no-coverage');

      const child = spawn('jest', jestArgs, {
        stdio: ['pipe', 'pipe', 'pipe'],
        shell: true,
        env: { ...process.env, NODE_ENV: 'test' }
      });

      child.stdout.on('data', (data) => { stdout += data.toString(); });
      child.stderr.on('data', (data) => { stderr += data.toString(); });

      child.on('close', (code) => {
        const duration = Date.now() - startTime;
        const output = stdout + stderr;
        resolve({ file: testFile, success: code === 0, duration, output, stdout, stderr });
      });

      child.on('error', () => {
        const duration = Date.now() - startTime;
        resolve({ file: testFile, success: false, duration, output: stderr || 'Runner error', stdout, stderr });
      });
    });
  }

  // Print colored status indicator
  printStatus(success: boolean, text: string) {
    const indicator = success ? `${colors.green}${colors.bold}✓${colors.reset}` : `${colors.red}${colors.bold}✗${colors.reset}`;
    console.log(`${indicator} ${text}`);
  }

  // Print test file result
  printTestResult(result: any) {
    const { file, success, duration } = result;
    const durationText = `${colors.dim}(${duration}ms)${colors.reset}`;
    const fileText = `${colors.cyan}${file}${colors.reset}`;
    this.printStatus(success, `${fileText} ${durationText}`);
    if (!success && result.output) {
      const lines = result.output.split('\n');
      lines.forEach(line => { if (line.trim()) console.log(`  ${colors.dim}${line}${colors.reset}`); });
    }
  }

  // Generate debug file for failed tests
  generateDebugFile() {
    const failedResults = this.testResults.filter(r => !r.success);
    if (failedResults.length === 0) return;
    let debugContent = '# Test Failure Analysis\n\n';
    debugContent += 'Analyze and address the following test failures:\n\n';
    failedResults.forEach((result, index) => {
      debugContent += `## Failed Test ${index + 1}: ${result.file}\n\n`;
      debugContent += '### Output:\n';
      debugContent += '```\n';
      debugContent += result.output;
      debugContent += '\n```\n\n';
      debugContent += `### Duration: ${result.duration}ms\n\n`;
      debugContent += '---\n\n';
    });
    debugContent += '## Summary\n\n';
    debugContent += `- Total failed tests: ${failedResults.length}\n`;
    debugContent += `- Failed test files: ${failedResults.map(r => r.file).join(', ')}\n`;
    debugContent += `- Generated: ${new Date().toISOString()}\n`;
    fs.writeFileSync('DEBUG_TESTS.md', debugContent);
    console.log(`\n${colors.yellow}📋 Debug file created: DEBUG_TESTS.md${colors.reset}`);
  }

  // Print comprehensive summary
  printSummary() {
    const duration = Date.now() - this.startTime;
    const totalFiles = this.testResults.length;
    console.log(`\n${colors.bold}═══════════════════════════════════════${colors.reset}`);
    console.log(`${colors.bold}${colors.white}           TEST SUMMARY${colors.reset}`);
    console.log(`${colors.bold}═══════════════════════════════════════${colors.reset}`);
    const allPassed = this.failedTests === 0;
    const statusColor = allPassed ? colors.green : colors.red;
    const statusText = allPassed ? 'ALL TESTS PASSED' : 'SOME TESTS FAILED';
    console.log(`${statusColor}${colors.bold}${statusText}${colors.reset}\n`);
    console.log(`${colors.green}✓ Passed:${colors.reset} ${colors.bold}${this.passedTests}${colors.reset}`);
    console.log(`${colors.red}✗ Failed:${colors.reset} ${colors.bold}${this.failedTests}${colors.reset}`);
    console.log(`${colors.blue}📁 Files:${colors.reset} ${colors.bold}${totalFiles}${colors.reset}`);
    console.log(`${colors.magenta}⏱  Duration:${colors.reset} ${colors.bold}${duration}ms${colors.reset}`);
    if (this.failedTests > 0) {
      console.log(`\n${colors.red}Failed test files:${colors.reset}`);
      this.testResults.filter(r => !r.success).forEach(r => console.log(`  ${colors.red}•${colors.reset} ${r.file}`));
    }
    console.log(`\n${colors.bold}═══════════════════════════════════════${colors.reset}`);
  }

  // Main runner method - optimized for parallel execution
  async run() {
    console.log(`${colors.bold}${colors.blue}🧪 qtests Test Runner - Parallel Mode${colors.reset}`);
    console.log(`${colors.dim}Discovering and running all tests...\n${colors.reset}`);
    
    // Check if specific test files were passed as arguments
    const args = process.argv.slice(2);
    const specificTestFiles = args.filter(arg => !arg.startsWith('--') && (arg.endsWith('.test.ts') || arg.endsWith('.test.js') || arg.endsWith('.spec.ts') || arg.endsWith('.spec.js')));
    
    const testFiles = specificTestFiles.length > 0 ? specificTestFiles : this.discoverTests();
    if (testFiles.length === 0) {
      console.log(`${colors.yellow}⚠  No test files found${colors.reset}`);
      console.log(`${colors.dim}Looked for files matching: ${TEST_PATTERNS.map(p => p.toString()).join(', ')}${colors.reset}`);
      process.exit(0);
    }
    console.log(`${colors.blue}Found ${testFiles.length} test file(s):${colors.reset}`);
    testFiles.forEach(file => console.log(`  ${colors.dim}•${colors.reset} ${file}`));
    console.log(`\n${colors.magenta}🚀 Running tests in parallel...${colors.reset}\n`);
    const cpuCount = os.cpus().length;
    const maxConcurrency = Math.min(testFiles.length, Math.max(4, cpuCount * 2));
    console.log(`${colors.dim}Max concurrency: ${maxConcurrency} workers (${cpuCount} CPU cores)${colors.reset}\n`);

    const runBatch = async (batch: string[]) => {
      const results = await Promise.all(batch.map(async (testFile) => {
        const result: any = await this.runTestFile(testFile);
        if (result.success) this.passedTests++; else this.failedTests++;
        this.printTestResult(result);
        return result;
      }));
      return results;
    };

    for (let i = 0; i < testFiles.length; i += maxConcurrency) {
      const batch = testFiles.slice(i, i + maxConcurrency);
      const batchResults = await runBatch(batch);
      this.testResults.push(...batchResults);
      if (i + maxConcurrency < testFiles.length) {
        console.log(`${colors.dim}Completed batch ${Math.floor(i / maxConcurrency) + 1}, starting next...${colors.reset}\n`);
      }
    }

    if (this.failedTests > 0) this.generateDebugFile();
    this.printSummary();
    process.exit(this.failedTests > 0 ? 1 : 0);
  }
}

// Run the test suite
const runner = new TestRunner();
runner.run().catch(error => {
  console.error(`${colors.red}Test runner error:${colors.reset}`, error);
  process.exit(1);
});

export default TestRunner;
