#!/usr/bin/env node
// GENERATED RUNNER: qtests-runner.mjs - auto-created by qtests TestGenerator
// Safe to delete; will be recreated as needed.
// Mirrors bin/qtests-ts-runner behavior (batching, DEBUG_TESTS.md, stable exits).
// Provides batching, summaries, and debug report creation for failing tests.

// qtests Test Runner - Feature-rich CLI with batching, summaries, and debug file
// Restores original functionality while ensuring Jest is launched with project config

import fs from 'fs';
import path from 'path';
// Unified runner: API-only, no child_process spawns
import os from 'os';
import { createRequire } from 'module';

// ANSI color codes for terminal output
const colors = {
  green: '\u001b[32m',
  red: '\u001b[31m',
  yellow: '\u001b[33m',
  blue: '\u001b[34m',
  magenta: '\u001b[35m',
  cyan: '\u001b[36m',
  white: '\u001b[37m',
  reset: '\u001b[0m',
  bold: '\u001b[1m',
  dim: '\u001b[2m'
};

// Test discovery patterns
const TEST_PATTERNS = [
  /\.test\.(js|ts|jsx|tsx)$/,
  /\.spec\.(js|ts|jsx|tsx)$/,
  /_test\.(js|ts|jsx|tsx)$/,
  /_spec\.(js|ts|jsx|tsx)$/
];

class TestRunner {
  constructor() {
    this.passedTests = 0;
    this.failedTests = 0;
    this.testResults = [];
    this.startTime = Date.now();
    // Mutex to ensure Jest API fallback runs sequentially to avoid conflicts
    this._apiMutex = Promise.resolve();
    // Lazily-initialized reference to Jest's runCLI for in-process fallback
    this._runCLI = null;
  }
  // Resolve a binary path from PATH, preferring earlier entries explicitly.
  resolveBin(binName) {
    try {
      const pathVar = process.env.PATH || process.env.Path || process.env.path || '';
      const parts = pathVar.split(path.delimiter).filter(Boolean);
      for (const dir of parts) {
        const candidate = path.join(dir, binName);
        try {
          const st = fs.statSync(candidate);
          if (st.isFile()) {
            try { fs.accessSync(candidate, fs.constants.X_OK); } catch {}
            return candidate;
          }
        } catch {}
      }
    } catch {}
    return null;
  }

  // Parse common truthy env values: '1', 'true', 'yes' (case-insensitive)
  isEnvTruthy(name) {
    const v = process.env[name];
    if (!v) return false;
    const s = String(v).trim().toLowerCase();
    return s === '1' || s === 'true' || s === 'yes';
  }

  // Discover all test files in the project (from current working directory)
  discoverTests(dir = process.cwd(), depth = 0, maxDepth = 10) {
    const testFiles = [];
    if (depth > maxDepth) return testFiles;
    try {
      const entries = fs.readdirSync(dir, { withFileTypes: true });
      for (const entry of entries) {
        const fullPath = path.join(dir, entry.name);
        // Skip node_modules, hidden directories, and common build output
        if (
          entry.name.startsWith('.') ||
          entry.name === 'node_modules' ||
          entry.name === 'demo' ||
          entry.name === 'dist' ||
          entry.name === 'build'
        ) {
          continue;
        }
        if (entry.isDirectory()) {
          testFiles.push(...this.discoverTests(fullPath, depth + 1, maxDepth));
        } else if (entry.isFile()) {
          const isTestFile = TEST_PATTERNS.some(pattern => pattern.test(entry.name));
          if (!isTestFile) continue;
          // Skip tests in __mocks__ and other ignored folders
          if (fullPath.includes(`${path.sep}__mocks__${path.sep}`)) continue;
          // In generated-tests, only include files that follow the GeneratedTest naming convention
          if (fullPath.includes(`${path.sep}generated-tests${path.sep}`) && !/GeneratedTest/.test(entry.name)) {
            continue;
          }
          testFiles.push(fullPath);
        }
      }
    } catch {
      // Silently skip directories we can't read
    }
    // Optional pattern filter via env to limit scope for large repos/CI
    const pattern = process.env.QTESTS_PATTERN;
    if (pattern) {
      try {
        const rx = new RegExp(pattern);
        return testFiles.filter(f => rx.test(f));
      } catch {
        return testFiles; // invalid pattern, ignore
      }
    }
    return testFiles;
  }

  // Run a single test file via Jest and capture output
  async runTestFile(testFile) {
    const spawnOnce = () => new Promise((resolve) => {
      const startTime = Date.now();
      let stdout = '';
      let stderr = '';
      let spawnHadError = false;

      // Always use Jest with project config and allow empty matches
      const configCandidates = [
        path.join(process.cwd(), 'config', 'jest.config.mjs'),
        path.join(process.cwd(), 'jest.config.mjs')
      ];
      const cfg = configCandidates.find(p => {
        try { return fs.existsSync(p); } catch { return false; }
      });
      const jestArgs = [];
      if (cfg) {
        jestArgs.push('--config', cfg);
      }
      jestArgs.push('--passWithNoTests');
      const fileWorkers = process.env.QTESTS_FILE_WORKERS ? String(process.env.QTESTS_FILE_WORKERS) : '4';
      const inBand = process.env.QTESTS_INBAND === '1' || process.env.QTESTS_INBAND === 'true';
      jestArgs.push(testFile);
      if (inBand) {
        jestArgs.push('--runInBand');
      } else {
        // Default per-file workers for performance without overwhelming sandbox envs
        jestArgs.push(`--maxWorkers=${fileWorkers}`);
      }
      jestArgs.push('--cache', '--no-coverage');

      // Record the exact Jest invocation args for debugging/tests. This is
      // helpful in restricted environments where spawning the child process
      // may be blocked, allowing assertions about intended behavior.
      try {
        const sinkPath = path.join(process.cwd(), 'runner-jest-args.json');
        fs.writeFileSync(sinkPath, JSON.stringify(jestArgs), 'utf8');
      } catch { /* best effort only */ }

      // In API-only mode we do not spawn per-file; simulate an immediate resolve.
      resolve({ file: testFile, success: false, duration: 0, output: 'API-only runner: per-file spawn disabled', stdout, stderr, spawnError: spawnHadError });
    });

    const first = await spawnOnce();
    const crashed = /A jest worker process [^\n]* crashed/i.test(first.output || '');
    const alreadyInBand = process.env.QTESTS_INBAND === '1' || process.env.QTESTS_INBAND === 'true';
    if (!first.success && crashed && !alreadyInBand) {
      console.error(`${colors.yellow}Retrying in-band due to worker crash: ${testFile}${colors.reset}`);
      process.env.QTESTS_INBAND = '1';
      return await this.runTestFile(testFile);
    }
    // Only fallback to in-process Jest API when the spawn itself failed (ENOENT/EACCES/EPERM)
    // and the environment explicitly allows API fallback.
    const shouldApiFallback = !first.success && (
      first.spawnError === true || /spawn\s+(ENOENT|EACCES|EPERM)/i.test(first.output || '')
    );
    const allowAPIFallback = this.isEnvTruthy('QTESTS_API_FALLBACK');
    if (shouldApiFallback && allowAPIFallback) {
      const start = Date.now();
      const fallback = await this._withAPILock(() => this._runTestFileViaAPI(testFile));
      return { ...fallback, duration: Date.now() - start };
    }
    return first;
  }

  // Ensure only one in-process Jest run executes at a time
  async _withAPILock(fn) {
    const prev = this._apiMutex;
    let resolveNext;
    this._apiMutex = new Promise(res => { resolveNext = res; });
    await prev.catch(() => {});
    try {
      const result = await fn();
      resolveNext();
      return result;
    } catch (err) {
      resolveNext();
      throw err;
    }
  }

  // Fallback: execute Jest via its programmatic API inside this Node process
  async _runTestFileViaAPI(testFile) {
    const startTime = Date.now();
    let stdout = '';
    let stderr = '';
    try {
      // Lazily require jest to avoid upfront overhead and allow CJS interop from ESM
      if (!this._runCLI) {
        const require = createRequire(import.meta.url);
        const jestModule = require('jest');
        this._runCLI = jestModule && jestModule.runCLI ? jestModule.runCLI : null;
      }
      if (!this._runCLI) {
        const msg = 'Jest API not available for fallback execution.';
        return { file: testFile, success: false, duration: Date.now() - startTime, output: msg, stdout, stderr };
      }

      // Determine config path
      const configCandidates = [
        path.join(process.cwd(), 'config', 'jest.config.mjs'),
        path.join(process.cwd(), 'jest.config.mjs')
      ];
      const cfg = configCandidates.find(p => {
        try { return fs.existsSync(p); } catch { return false; }
      });

      // Build argv for runCLI
      const argv = {
        runInBand: true,
        passWithNoTests: true,
        silent: false,
        runTestsByPath: true,
        _: [testFile],
        $0: 'jest'
      };
      if (cfg) {
        argv.config = cfg; // jest accepts path string for --config
      }

      // Capture console output during runCLI to include in debug file
      const origLog = console.log;
      const origErr = console.error;
      console.log = (...args) => { try { stdout += args.join(' ') + '\n'; } catch {} origLog.apply(console, args); };
      console.error = (...args) => { try { stderr += args.join(' ') + '\n'; } catch {} origErr.apply(console, args); };
      try {
        const { results } = await this._runCLI(argv, [process.cwd()]);
        const duration = Date.now() - startTime;
        const success = results && results.success === true;
        const mergedOut = (stdout + (stderr ? ('\n' + stderr) : '')).trim();
        return {
          file: testFile,
          success,
          duration,
          output: mergedOut || (success ? '' : 'Jest API run failed with no output.'),
          stdout,
          stderr
        };
      } finally {
        console.log = origLog;
        console.error = origErr;
      }
    } catch (err) {
      const duration = Date.now() - startTime;
      const msg = (err && (err.stack || err.message)) || 'Jest API fallback error';
      return { file: testFile, success: false, duration, output: msg, stdout, stderr };
    }
  }

  // API-only: execute all tests in-process via Jest's programmatic API (no child spawns)
  async runAllViaAPI(testFiles) {
    let stdout = '';
    let stderr = '';
    try {
      // Lazily require jest to avoid upfront overhead and allow CJS interop from ESM
      if (!this._runCLI) {
        const require = createRequire(import.meta.url);
        const jestModule = require('jest');
        this._runCLI = jestModule && jestModule.runCLI ? jestModule.runCLI : null;
      }
      if (!this._runCLI) {
        const msg = 'Jest API not available for fallback execution.';
        console.error(msg);
        process.exit(1);
      }

      // Determine config path
      const configCandidates = [
        path.join(process.cwd(), 'config', 'jest.config.mjs'),
        path.join(process.cwd(), 'jest.config.mjs')
      ];
      const cfg = configCandidates.find(p => {
        try { return fs.existsSync(p); } catch { return false; }
      });

      const runInBand = this.isEnvTruthy('QTESTS_INBAND') || (!process.env.QTESTS_FILE_WORKERS && !process.env.QTESTS_CONCURRENCY);
      const argv = {
        runInBand,
        passWithNoTests: true,
        silent: false,
        runTestsByPath: true,
        cache: true,
        coverage: false,
        _: testFiles,
        $0: 'jest'
      };
      const workerStr = process.env.QTESTS_FILE_WORKERS || process.env.QTESTS_CONCURRENCY;
      const maxW = workerStr ? parseInt(String(workerStr), 10) : undefined;
      if (!runInBand && Number.isFinite(maxW) && maxW > 0) {
        argv.maxWorkers = maxW;
      }
      if (cfg) argv.config = cfg; // jest accepts path string for --config

      // Record intended Jest invocation for debugging/tests
      try {
        const intendedArgs = [];
        if (cfg) intendedArgs.push('--config', cfg);
        intendedArgs.push('--passWithNoTests');
        if (runInBand) intendedArgs.push('--runInBand');
        else if (Number.isFinite(maxW) && maxW > 0) intendedArgs.push(`--maxWorkers=${maxW}`);
        intendedArgs.push('--cache', '--no-coverage');
        fs.writeFileSync(path.join(process.cwd(), 'runner-jest-args.json'), JSON.stringify(intendedArgs), 'utf8');
      } catch { /* best effort only */ }

      // Capture console output during runCLI to include in debug file
      const origLog = console.log;
      const origErr = console.error;
      console.log = (...args) => { try { stdout += args.join(' ') + '\n'; } catch {} origLog.apply(console, args); };
      console.error = (...args) => { try { stderr += args.join(' ') + '\n'; } catch {} origErr.apply(console, args); };
      try {
        const { results } = await this._runCLI(argv, [process.cwd()]);
        for (const tr of (results.testResults || [])) {
          const file = tr.testFilePath || 'unknown';
          const success = (tr.numFailingTests || 0) === 0 && !tr.failureMessage;
          const duration = tr.perfStats && tr.perfStats.end && tr.perfStats.start ? (tr.perfStats.end - tr.perfStats.start) : 0;
          const output = tr.failureMessage || '';
          const rec = { file, success, duration, output, stdout: '', stderr: '' };
          if (rec.success) this.passedTests++; else this.failedTests++;
          this.testResults.push(rec);
          this.printTestResult(rec);
        }
      } finally {
        console.log = origLog;
        console.error = origErr;
      }
    } catch (err) {
      const msg = (err && (err.stack || err.message)) || 'Jest API run error';
      console.error(msg);
      this.failedTests++;
    }
    if (this.failedTests > 0) this.generateDebugFile();
    this.printSummary();
    process.exit(this.failedTests > 0 ? 1 : 0);
  }

  // Print colored status indicator
  printStatus(success, text) {
    const indicator = success ? `${colors.green}${colors.bold}✓${colors.reset}` : `${colors.red}${colors.bold}✗${colors.reset}`;
    console.log(`${indicator} ${text}`);
  }

  // Print test file result
  printTestResult(result) {
    const { file, success, duration } = result;
    const durationText = `${colors.dim}(${duration}ms)${colors.reset}`;
    const fileText = `${colors.cyan}${file}${colors.reset}`;
    this.printStatus(success, `${fileText} ${durationText}`);
    if (!success && result.output) {
      const lines = result.output.split('\n');
      lines.forEach(line => { if (line.trim()) console.log(`  ${colors.dim}${line}${colors.reset}`); });
    }
  }

  // Generate debug file for failed tests
  generateDebugFile() {
    const failedResults = this.testResults.filter(r => !r.success);
    if (failedResults.length === 0) return;
    // Allow CI/tests to suppress creating a repo-root debug artifact when undesired
    if (this.isEnvTruthy('QTESTS_SUPPRESS_DEBUG') || this.isEnvTruthy('QTESTS_NO_DEBUG_FILE')) {
      return;
    }
    // Allow overriding the debug file output path to avoid cross-test interference
    const debugFilePath = (process.env.QTESTS_DEBUG_FILE && String(process.env.QTESTS_DEBUG_FILE).trim()) || 'DEBUG_TESTS.md';
    let debugContent = '# Test Failure Analysis\n\n';
    debugContent += 'Analyze and address the following test failures:\n\n';
    failedResults.forEach((result, index) => {
      debugContent += `## Failed Test ${index + 1}: ${result.file}\n\n`;
      debugContent += '### Output:\n';
      debugContent += '```\n';
      debugContent += result.output;
      debugContent += '\n```\n\n';
      debugContent += `### Duration: ${result.duration}ms\n\n`;
      debugContent += '---\n\n';
    });
    debugContent += '## Summary\n\n';
    debugContent += `- Total failed tests: ${failedResults.length}\n`;
    debugContent += `- Failed test files: ${failedResults.map(r => r.file).join(', ')}\n`;
    debugContent += `- Generated: ${new Date().toISOString()}\n`;
    try {
      fs.writeFileSync(debugFilePath, debugContent);
    } catch {
      // Best-effort; ignore write errors in restricted environments
    }
    if (!this.isEnvTruthy('QTESTS_SILENT')) {
      console.log(`\n${colors.yellow}📋 Debug file created: ${debugFilePath}${colors.reset}`);
    }
  }

  // Print comprehensive summary
  printSummary() {
    const duration = Date.now() - this.startTime;
    const totalFiles = this.testResults.length;
    console.log(`\n${colors.bold}═══════════════════════════════════════${colors.reset}`);
    console.log(`${colors.bold}${colors.white}           TEST SUMMARY${colors.reset}`);
    console.log(`${colors.bold}═══════════════════════════════════════${colors.reset}`);
    const allPassed = this.failedTests === 0;
    const statusColor = allPassed ? colors.green : colors.red;
    const statusText = allPassed ? 'ALL TESTS PASSED' : 'SOME TESTS FAILED';
    console.log(`${statusColor}${colors.bold}${statusText}${colors.reset}\n`);
    console.log(`${colors.green}✓ Passed:${colors.reset} ${colors.bold}${this.passedTests}${colors.reset}`);
    console.log(`${colors.red}✗ Failed:${colors.reset} ${colors.bold}${this.failedTests}${colors.reset}`);
    console.log(`${colors.blue}📁 Files:${colors.reset} ${colors.bold}${totalFiles}${colors.reset}`);
    console.log(`${colors.magenta}⏱  Duration:${colors.reset} ${colors.bold}${duration}ms${colors.reset}`);
    if (this.failedTests > 0) {
      console.log(`\n${colors.red}Failed test files:${colors.reset}`);
      this.testResults.filter(r => !r.success).forEach(r => console.log(`  ${colors.red}•${colors.reset} ${r.file}`));
    }
    console.log(`\n${colors.bold}═══════════════════════════════════════${colors.reset}`);
  }

  // Main runner method - API-only execution (no child spawns)
  async run() {
    if (!this.isEnvTruthy('QTESTS_SILENT')) {
      console.log(`${colors.bold}${colors.blue}🧪 qtests Test Runner - API Mode${colors.reset}`);
      console.log(`${colors.dim}Discovering and running all tests...\n${colors.reset}`);
    }
    const files = this.discoverTests();
    if (files.length === 0) {
      if (!this.isEnvTruthy('QTESTS_SILENT')) {
        console.log(`${colors.yellow}⚠  No test files found${colors.reset}`);
        console.log(`${colors.dim}Looked for files matching: ${TEST_PATTERNS.map(p => p.toString()).join(', ')}${colors.reset}`);
      }
      process.exit(0);
    }
    if (!this.isEnvTruthy('QTESTS_SILENT')) {
      console.log(`${colors.blue}Found ${files.length} test file(s):${colors.reset}`);
      files.forEach(file => console.log(`  ${colors.dim}•${colors.reset} ${file}`));
      console.log(``);
    }
    await this.runAllViaAPI(files);
  }
}

// Run the test suite
const runner = new TestRunner();
runner.run().catch(error => {
  console.error(`${colors.red}Test runner error:${colors.reset}`, error);
  process.exit(1);
});
